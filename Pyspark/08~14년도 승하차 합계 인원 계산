from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType

# Spark 세션 생성
spark = SparkSession.builder.appName("SubwayPassengerAnalysis").getOrCreate()

# CSV 파일을 DataFrame으로 로드
file_path = 'hdfs:///user/maria_dev/test_1/서울교통공사_####년.csv'
df = spark.read.csv(file_path, header=True)

# 컬럼 이름의 앞뒤 공백 제거
df = df.select([F.col(column).alias(column.strip()) for column in df.columns])

# '날짜' 및 '역명' 컬럼의 앞뒤 공백 제거
df = df.withColumn("날짜", F.trim(df["날짜"]))
df = df.withColumn("역명", F.trim(df["역명"]))

# 문자열 숫자를 정수로 변환하는 사용자 정의 함수(UDF) 정의
to_int_udf = F.udf(lambda x: int(x.replace(",", "")) if x.replace(",", "").isdigit() else 0, IntegerType())

# 관련 컬럼을 정수로 변환
time_columns = df.columns[4:]
for column in time_columns:
    df = df.withColumn(column, to_int_udf(column))

# 각 역별 승객 수를 합산하여 'result' 컬럼 생성
df = df.withColumn("result", sum(df[column] for column in time_columns))

# '날짜' 및 '역명'으로 그룹화하고 각 시간대 및 'result'에 대한 합산을 수행하여 결과 DataFrame 생성
result_df = df.groupBy("날짜", "역명").agg(
    F.sum("05~06").alias("05~06"),
    F.sum("06~07").alias("06~07"),
    F.sum("07~08").alias("07~08"),
    F.sum("08~09").alias("08~09"),
    F.sum("09~10").alias("09~10"),
    F.sum("10~11").alias("10~11"),
    F.sum("11~12").alias("11~12"),
    F.sum("12~13").alias("12~13"),
    F.sum("13~14").alias("13~14"),
    F.sum("14~15").alias("14~15"),
    F.sum("15~16").alias("15~16"),
    F.sum("16~17").alias("16~17"),
    F.sum("17~18").alias("17~18"),
    F.sum("18~19").alias("18~19"),
    F.sum("19~20").alias("19~20"),
    F.sum("20~21").alias("20~21"),
    F.sum("21~22").alias("21~22"),
    F.sum("22~23").alias("22~23"),
    F.sum("23~24").alias("23~24"),
    F.sum("24~").alias("24~"),
    F.sum("result").alias("total_passengers")
)

# '날짜' 및 '역명'을 기준으로 정렬
result_df = result_df.orderBy("날짜", "역명")

# 하나의 파티션으로 병합하고 결과 DataFrame을 UTF-8 인코딩으로 HDFS에 CSV 파일로 저장
result_file_path = 'hdfs:///user/maria_dev/test_2/서울교통통사_####년_합계.csv'
result_df.coalesce(1).write.option("header", "true").option("encoding", "UTF-8").csv(result_file_path, mode='overwrite')

# Spark 세션 종료
spark.stop()
