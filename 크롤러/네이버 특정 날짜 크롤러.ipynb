{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgOH3GF1JFC4YdkgfPfwbI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgXwQ3NbNHfL","executionInfo":{"status":"ok","timestamp":1701522784466,"user_tz":-540,"elapsed":8321,"user":{"displayName":"김민서/융합소프트웨어학부","userId":"15333968746817765257"}},"outputId":"21f9aa71-292a-4e46-faaf-201c38b4eb3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["검색할 지하철역을 입력해주세요:증산역\n","궁금하신 날짜를 입력해주세요 (YYYY-MM-DD)2023-03-15\n","생성된 URL:  https://search.naver.com/search.naver?where=news&query=증산역&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2023.03.15&de=2023.03.15&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom2023.03.15to2023.03.15&is_sug_officeid=0&office_category=0&service_area=0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:00<00:00, 1458.63it/s]\n","100%|██████████| 1/1 [00:00<00:00,  2.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[뉴스 제목]\n","['오늘 새벽 서울 증산역 인근서 마주오던 차량 충돌']\n","\n","[뉴스 링크]\n","['https://n.news.naver.com/mnews/article/056/0011445529?sid=102']\n","\n","[뉴스 내용]\n","[\"[ 오늘(15일) 새벽 1시 반쯤 서울 은평구 증산역 인근에서 마주 오던 승합차와 승용차가 부딪치는 사고가 일어났습니다. 이 사고로 두 차량의 운전자가 각각 의식이 있는 상태로 병원으로 옮겨졌습니다. 경찰은 운전자들에 대해 음주 여부를 검사했지만, 음주운전은 아니었던 것으로 파악됐습니다. 경찰은 정확한 사고 경위를 조사하고 있습니다.[사진 출처 : 시청자 강대성 님 제공]■ 제보하기▷ 카카오톡 : 'KBS제보' 검색▷ 전화 : 02-781-1234▷ 이메일 : kbs1234@kbs.co.kr▷ 뉴스홈페이지 : https://goo.gl/4bWbkG ]\"]\n","newsTitle:  1\n","newsUrl:  1\n","newsContents:  1\n","newsDates:  1\n","중복 제거 후 행 개수:  1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from bs4 import BeautifulSoup\n","import requests\n","import re\n","import datetime\n","from tqdm import tqdm\n","import sys\n","\n","# 크롤링할 URL 생성하는 함수 만들기(검색어, 날짜)\n","def makeUrl(search, date):\n","    formattedDate = date.replace(\"-\", \".\")\n","    url = f\"https://search.naver.com/search.naver?where=news&query={search}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={formattedDate}&de={formattedDate}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{formattedDate}to{formattedDate}&is_sug_officeid=0&office_category=0&service_area=0\"\n","    print(\"생성된 URL: \", url)\n","    return url\n","\n","# HTML에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n","def newsAttrsCrawler(articles, attrs):\n","    attrsContent = []\n","    for i in articles:\n","        attrsContent.append(i.attrs[attrs])\n","    return attrsContent\n","\n","# ConnectionError 방지\n","headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n","\n","# HTML 생성해서 기사 크롤링하는 함수 만들기(url): 링크를 반환\n","def articlesCrawler(url):\n","    # HTML 불러오기\n","    originalHtml = requests.get(url)\n","    html = BeautifulSoup(originalHtml.text, \"html.parser\")\n","\n","    urlNaver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")  # CSS 선택자를 통해 뉴스 링크 요소 선택\n","    url = newsAttrsCrawler(urlNaver, 'href')  # 선택된 요소들 중 href(링크) 속성만 추출하는 함수 호출\n","    return url\n","\n","##### 뉴스 크롤링 시작 #####\n","\n","# 검색어 입력\n","search = input(\"검색할 지하철역을 입력해주세요:\")\n","date = input(\"궁금하신 날짜를 입력해주세요 (YYYY-MM-DD)\")\n","\n","# Naver URL 생성\n","url = makeUrl(search, date)\n","\n","# 뉴스 크롤러 실행\n","newsTitles = []\n","newsUrl = []\n","newsContents = []\n","newsDates = []\n","\n","url = articlesCrawler(url)\n","newsUrl.append(url)\n","\n","# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n","def makeList(newList, content):\n","    for i in content:\n","        for j in i:\n","            newList.append(j)\n","    return newList\n","\n","# 제목, 링크, 내용 담을 리스트 생성\n","newsUrl1 = []\n","\n","# 1차원 리스트로 만들기(내용 제외)\n","makeList(newsUrl1, newsUrl)\n","\n","# 신뢰도를 위해 NAVER 뉴스만 남기기\n","finalUrls = []\n","for i in tqdm(range(len(newsUrl1))):\n","    if \"news.naver.com\" in newsUrl1[i]:\n","        finalUrls.append(newsUrl1[i])\n","    else:\n","        pass\n","\n","# 뉴스 내용 크롤링 (진행률 표시)\n","for i in tqdm(finalUrls):\n","    # 각 기사 HTML get하기\n","    news = requests.get(i, headers=headers)\n","    newsHtml = BeautifulSoup(news.text, \"html.parser\")\n","\n","    # 뉴스 제목 가져오기\n","    title = newsHtml.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n","    if title is None:\n","        title = newsHtml.select_one(\"#content > div.end_ct > div > h2\")\n","\n","    # 뉴스 본문 가져오기\n","    content = newsHtml.select(\"article#dic_area\")\n","    if content == []:\n","        content = newsHtml.select(\"#articeBody\")\n","\n","    # 기사 텍스트만 가져오기\n","    # list 합치기\n","    content = ''.join(str(content))\n","\n","    # 정규 표현식 활용 HTML 태그 제거 및 텍스트 다듬기\n","    pattern1 = '<[^>]*>'\n","    title = re.sub(pattern=pattern1, repl='', string=str(title))\n","    content = re.sub(pattern=pattern1, repl='', string=content)\n","    content = re.sub(r'\\n+', ' ', content)\n","    newsTitles.append(title)\n","    newsContents.append(content)\n","\n","    try:\n","        htmlDate = newsHtml.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n","        newsDate = htmlDate.attrs['data-date-time']\n","    except AttributeError:\n","        newsDate = newsHtml.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n","        newsDate = re.sub(pattern=pattern1, repl='', string=str(newsDate))\n","    # 날짜 가져오기\n","    newsDates.append(newsDate)\n","\n","print(\"\\n[뉴스 제목]\")\n","print(newsTitles)\n","print(\"\\n[뉴스 링크]\")\n","print(finalUrls)\n","print(\"\\n[뉴스 내용]\")\n","print(newsContents)\n","\n","print('newsTitle: ', len(newsTitles))\n","print('newsUrl: ', len(finalUrls))\n","print('newsContents: ', len(newsContents))\n","print('newsDates: ', len(newsDates))\n","\n","### 데이터 프레임으로 만들기 ###\n","import pandas as pd\n","\n","# 데이터 프레임 만들기\n","newsDf = pd.DataFrame({'date': newsDates, 'title': newsTitles, 'link': finalUrls, 'content': newsContents})\n","\n","# 중복 행 지우기\n","newsDf = newsDf.drop_duplicates(keep='first', ignore_index=True)\n","print(\"중복 제거 후 행 개수: \", len(newsDf))\n","\n","# 데이터 프레임 저장\n","now = datetime.datetime.now()\n","newsDf['date'] = pd.to_datetime(newsDf['date']).dt.strftime('%Y-%m-%d')  # 날짜 형식 변경\n","filename = '{}_{}.csv'.format(search, date)\n","newsDf.to_csv(filename, encoding='utf-8-sig', index=False)\n"]}]}