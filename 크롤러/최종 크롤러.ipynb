from bs4 import BeautifulSoup
import requests
import re
import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

dfs_list = []

# ConnectionError 방지를 위한 헤더
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102", "referer": "https://www.youtube.com/watch?v=eap62CrRtgg"}

# URL 생성 함수
def makeUrl(search, date):
    url = f"https://search.naver.com/search.naver?where=news&query={search}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={date}&de={date}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{date}to{date}&is_sug_officeid=0&office_category=0&service_area=0"
    return url

# 기사를 크롤하고 링크를 추출하는 함수
def articlesCrawler(html, url):
    soup = BeautifulSoup(html, "html.parser")
    urlNaver_elements = soup.select("div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info")  # 여긴 완료

    # href 속성 추출
    urls = [element.get('href') for element in urlNaver_elements if element.get('href')]

    return urls

# 뉴스 제목을 크롤하는 함수
def crawl_titles(html, finalUrls):
    newsTitles = set()  # 중복을 허용하지 않는 set을 사용

    for url in tqdm(finalUrls):
        # 각 기사 HTML 파싱하기
        newsHtml = BeautifulSoup(html, "html.parser")
        news_items = newsHtml.select("div.news_area")

        # 뉴스 제목 가져오기
        for news_item in news_items:
            title_element = news_item.select_one("a.news_tit")
            if title_element:
                title_text = title_element.get_text(strip=True)
                # 선택적으로 정규 표현식을 사용하여 HTML 태그 제거
                pattern = '<[^>]*>'
                title_text = re.sub(pattern=pattern, repl='', string=title_text)
                newsTitles.add("[NEWS]"+title_text)  # set에 추가

    return newsTitles

def crawl_titles2(html, finalUrls):
    newsTitles = set()

    for i in tqdm(finalUrls):
        newsHtml = BeautifulSoup(html, "html.parser")

        titles = newsHtml.select('a.title_link._cross_trigger')  # 필요에 따라 셀렉터 조정
        for title in titles:
            title_text = re.sub(pattern='<[^>]*>', repl='', string=str(title))
            newsTitles.add("[VIEW]"+title_text)

    return newsTitles

# CSV 파일에서 데이터 읽기
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/BDP/서울교통공사_2022년_정제.csv', encoding='UTF-8')

# 각 행을 처리하는 함수
def process_row(index, row):
    search = row['station'].split('(')[0]
    date = row['date']
    formattedDate = date.replace("-", ".")
    url = makeUrl(search, formattedDate)

    # HTML 내용을 한 번만 가져오기
    originalHtml = requests.get(url, headers=headers)
    html = originalHtml.text

    # 기사 링크 추출
    article_urls = articlesCrawler(html, url)

    # NAVER 뉴스를 위한 필터링
    finalUrls = [u for u in article_urls if "news.naver.com" in u]

    # 원본 출처에서 URL이 없는 경우
    if not finalUrls:
        # 다른 사이트로 새로운 요청 생성
        new_url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={search}&nso=p%3Afrom{formattedDate}to{formattedDate}"  # 다른 사이트의 URL로 대체
        new_html = requests.get(new_url, headers=headers).text

        # 새로운 HTML에서 정보 추출
        new_soup = BeautifulSoup(new_html, "html.parser")
        title_links = new_soup.select('li.sh_blog_top > dl > dt > a')
        new_urls = [link.get('href') for link in new_soup.select('#main_pack > section > more-contents > div > ul > li > div > div.detail_box > div.title_area a')][:5]
        # 새로운 HTML에서 제목 추출
        new_titles = crawl_titles2(new_html, new_urls)

        # 새 데이터로 DataFrame 생성
        data = {'station': search, 'date': date, 'title': '\n'.join(new_titles), 'link': '\n'.join(new_urls)}
        newsDf = pd.DataFrame([data])
        return newsDf

    # 원본 출처를 사용하여 뉴스 제목 크롤
    newsTitles = crawl_titles(html, finalUrls)

    # 제목 연결 및 DataFrame 생성
    concatenated_titles = '\n'.join(newsTitles)
    data = {'station': search, 'date': date, 'title': concatenated_titles, 'link': '\n'.join(finalUrls)}
    newsDf = pd.DataFrame([data])

    return newsDf

# ThreadPoolExecutor를 사용한 멀티스레딩
with ThreadPoolExecutor(max_workers=5) as executor:
    # 크롤할 행 수를 입력하도록 사용자에게 요청
    num_rows_to_crawl = int(input("크롤할 행 수를 입력하세요: "))
    
    # 지정된 행 수만큼 처리
    futures = [executor.submit(process_row, index, row) for index, row in df.head(num_rows_to_crawl).iterrows()]

    # DataFrame을 연결하고 CSV로 저장할 때 tqdm을 사용하여 진행률 표시
    final_df = pd.concat(tqdm(futures, total=num_rows_to_crawl), ignore_index=True)
    final_df.to_csv("이상치 결과-뉴스.csv", encoding='utf-8-sig', index=False)

# 필요한 대로 DataFrame을 출력하거나 사용
